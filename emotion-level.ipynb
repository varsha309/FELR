{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-10T11:57:32.222888Z","iopub.execute_input":"2023-05-10T11:57:32.223248Z","iopub.status.idle":"2023-05-10T11:57:32.252055Z","shell.execute_reply.started":"2023-05-10T11:57:32.223218Z","shell.execute_reply":"2023-05-10T11:57:32.250896Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/face-emotion-level/fear.csv\n/kaggle/input/face-emotion-level/surprise.csv\n/kaggle/input/face-emotion-level/happy.csv\n/kaggle/input/face-emotion-level/sad.csv\n/kaggle/input/face-emotion-level/angry.csv\n/kaggle/input/face-emotion-level/emotion_level.csv\n/kaggle/input/face-emotion-level/disgust.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\n\n# !pip install scikitplot\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.utils import np_utils","metadata":{"execution":{"iopub.status.busy":"2023-05-10T11:57:32.254187Z","iopub.execute_input":"2023-05-10T11:57:32.254882Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"angry_df = pd.read_csv('/kaggle/input/face-emotion-level/angry.csv')\nangry_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disgust_df = pd.read_csv('/kaggle/input/face-emotion-level/disgust.csv')\ndisgust_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fear_df = pd.read_csv('/kaggle/input/face-emotion-level/fear.csv')\nfear_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happy_df = pd.read_csv('/kaggle/input/face-emotion-level/happy.csv')\nhappy_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sad_df = pd.read_csv('/kaggle/input/face-emotion-level/sad.csv')\nsad_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"surprise_df = pd.read_csv('/kaggle/input/face-emotion-level/surprise.csv')\nsurprise_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"angry_df.emotion_level.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=angry_df.emotion_level)\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"angry_img_array = angry_df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nangry_img_array = np.stack(angry_img_array, axis=0)\n\ndisgust_img_array = disgust_df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\ndisgust_img_array = np.stack(disgust_img_array, axis=0)\n\nfear_img_array = fear_df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nfear_img_array = np.stack(fear_img_array, axis=0)\n\nhappy_img_array = happy_df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nhappy_img_array = np.stack(happy_img_array, axis=0)\n\nsad_img_array = sad_df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nsad_img_array = np.stack(sad_img_array, axis=0)\n\nsurprise_img_array = surprise_df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nsurprise_img_array = np.stack(surprise_img_array, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\nangry_img_labels = le.fit_transform(angry_df.emotion_level)\nangry_img_labels = np_utils.to_categorical(angry_img_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\ndisgust_img_labels = le.fit_transform(disgust_df.emotion_level)\ndisgust_img_labels = np_utils.to_categorical(disgust_img_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\nfear_img_labels = le.fit_transform(fear_df.emotion_level)\nfear_img_labels = np_utils.to_categorical(fear_img_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\nhappy_img_labels = le.fit_transform(happy_df.emotion_level)\nhappy_img_labels = np_utils.to_categorical(happy_img_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\nsad_img_labels = le.fit_transform(sad_df.emotion_level)\nsad_img_labels = np_utils.to_categorical(sad_img_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\nsurprise_img_labels = le.fit_transform(surprise_df.emotion_level)\nsurprise_img_labels = np_utils.to_categorical(surprise_img_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_angry, X_valid_angry, y_train_angry, y_valid_angry = train_test_split(angry_img_array, angry_img_labels, shuffle=True, stratify=angry_img_labels, test_size=0.1, random_state=42)\n\nX_train_disgust, X_valid_disgust, y_train_disgust, y_valid_disgust = train_test_split(disgust_img_array, disgust_img_labels, shuffle=True, stratify=disgust_img_labels, test_size=0.1, random_state=42)\n\nX_train_fear, X_valid_fear, y_train_fear, y_valid_fear = train_test_split(fear_img_array, fear_img_labels, shuffle=True, stratify=fear_img_labels, test_size=0.1, random_state=42)\n\nX_train_happy, X_valid_happy, y_train_happy, y_valid_happy = train_test_split(happy_img_array, happy_img_labels, shuffle=True, stratify=happy_img_labels, test_size=0.1, random_state=42)\n\nX_train_sad, X_valid_sad, y_train_sad, y_valid_sad = train_test_split(sad_img_array, sad_img_labels, shuffle=True, stratify=sad_img_labels, test_size=0.1, random_state=42)\n\nX_train_surprise, X_valid_surprise, y_train_surprise, y_valid_surprise = train_test_split(surprise_img_array, surprise_img_labels, shuffle=True, stratify=surprise_img_labels, test_size=0.1, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_angry.shape, X_valid_angry.shape, y_train_angry.shape, y_valid_angry.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_width = X_train_angry.shape[1]\nimg_height = X_train_angry.shape[2]\nimg_depth = X_train_angry.shape[3]\nnum_classes = y_train_angry.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_angry = X_train_angry / 255.\nX_valid_angry = X_valid_angry / 255.\n\nX_train_disgust = X_train_disgust / 255.\nX_valid_disgust = X_valid_disgust / 255.\n\nX_train_fear = X_train_fear / 255.\nX_valid_fear = X_valid_fear / 255.   \n\nX_train_happy = X_train_happy / 255.\nX_valid_happy = X_valid_happy / 255.  \n\nX_train_sad = X_train_sad / 255.\nX_valid_sad = X_valid_sad / 255.  \n\nX_train_surprise = X_train_surprise / 255.\nX_valid_surprise = X_valid_surprise / 255.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_net(optim):\n    net = Sequential(name='DCNN')\n    \n    net.add(Conv2D(filters=64,kernel_size=(5,5),input_shape=(img_width, img_height, img_depth),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d1_1'))\n    net.add(BatchNormalization(name='batchnorm1_1'))\n    net.add(Conv2D(filters=64,kernel_size=(5,5),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d1_2'))\n    net.add(BatchNormalization(name='batchnorm1_2'))\n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d1_1'))\n    net.add(Dropout(0.4, name='dropout1_1'))\n    \n    net.add(Conv2D(filters=128,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d2_1'))\n    net.add(BatchNormalization(name='batchnorm2_1'))\n    net.add(Conv2D(filters=128,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d2_2'))\n    net.add(BatchNormalization(name='batchnorm2_2'))\n    net.add(Conv2D(filters=128,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d2_3'))\n    net.add(BatchNormalization(name='batchnorm2_3')) \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d2_1'))\n    net.add(Dropout(0.4, name='dropout2_1'))\n    \n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d3_1'))\n    net.add(BatchNormalization(name='batchnorm3_1'))\n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d3_2'))\n    net.add(BatchNormalization(name='batchnorm3_2'))\n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d3_3'))\n    net.add(BatchNormalization(name='batchnorm3_3'))\n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d3_1'))\n    net.add(Dropout(0.5, name='dropout3_1'))\n    \n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d4_1'))\n    net.add(BatchNormalization(name='batchnorm4_1'))\n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d4_2'))\n    net.add(BatchNormalization(name='batchnorm4_2'))\n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d4_3'))\n    net.add(BatchNormalization(name='batchnorm4_3'))\n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d4_1'))\n    net.add(Dropout(0.5, name='dropout4_1'))\n    \n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d5_1'))\n    net.add(BatchNormalization(name='batchnorm5_1'))\n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d5_2'))\n    net.add(BatchNormalization(name='batchnorm5_2'))\n    net.add(Conv2D(filters=256,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d5_3'))\n    net.add(BatchNormalization(name='batchnorm5_3'))\n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d5_1'))\n    net.add(Dropout(0.5, name='dropout5_1'))\n    \n    net.add(Flatten(name='flatten'))\n        \n    net.add(Dense(128,activation='elu',kernel_initializer='he_normal',name='dense_1'))\n    net.add(BatchNormalization(name='batchnorm_7'))\n    \n    net.add(Dropout(0.6, name='dropout7_1'))\n    \n    net.add(Dense(num_classes,activation='softmax',name='out_layer'))\n    \n    net.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n    \n    net.summary()\n    \n    return net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32 #batch size of 32 performs the best.\nepochs = 30\noptims = [\n    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n    optimizers.Adam(0.01),\n]\n\nangry_model = build_net(optims[1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_angry = angry_model.fit(\n    X_train_angry, \n    y_train_angry, \n    batch_size=batch_size,\n    validation_data=(X_valid_angry,y_valid_angry),\n    steps_per_epoch=len(X_train_angry) / batch_size,\n    epochs=epochs,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(x=history_angry.epoch, y=history_angry.history['accuracy'], label='train')\nsns.lineplot(x=history_angry.epoch, y=history_angry.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(x=history_angry.epoch, y=history_angry.history['loss'], label='train')\nsns.lineplot(x=history_angry.epoch, y=history_angry.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_angry_dcnn.png')\npyplot.show()\n\nprint(\"Final Accuracy of angry after 100 epochs will be: \", round(history_angry.history['accuracy'][99]*100,2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x_angry=angry_model.predict(X_valid_angry) \nyhat_valid_angry=np.argmax(predict_x_angry,axis=1)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid_angry, axis=1), yhat_valid_angry, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_angry_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid_angry, axis=1) != yhat_valid_angry)}\\n\\n')\nprint(classification_report(np.argmax(y_valid_angry, axis=1), yhat_valid_angry))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\n\njoblib.dump(angry_model, \"/kaggle/working/angry.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disgust_model = build_net(optims[1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_disgust = disgust_model.fit(\n    X_train_disgust, \n    y_train_disgust, \n    batch_size=batch_size,\n    validation_data=(X_valid_disgust,y_valid_disgust),\n    steps_per_epoch=len(X_train_disgust) / batch_size,\n    epochs=epochs,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(x=history_disgust.epoch, y=history_disgust.history['accuracy'], label='train')\nsns.lineplot(x=history_disgust.epoch, y=history_disgust.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(x=history_disgust.epoch, y=history_disgust.history['loss'], label='train')\nsns.lineplot(x=history_disgust.epoch, y=history_disgust.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_disgust_dcnn.png')\npyplot.show()\n\nprint(\"Final Accuracy of disgust after 100 epochs will be: \", round(history_disgust.history['accuracy'][99]*100,2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x_disgust=disgust_model.predict(X_valid_disgust) \nyhat_valid_disgust=np.argmax(predict_x_disgust,axis=1)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid_disgust, axis=1), yhat_valid_disgust, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_disgust_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid_disgust, axis=1) != yhat_valid_disgust)}\\n\\n')\nprint(classification_report(np.argmax(y_valid_disgust, axis=1), yhat_valid_disgust))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(disgust_model, \"/kaggle/working/disgust.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fear_model = build_net(optims[1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_fear = fear_model.fit(\n    X_train_fear, \n    y_train_fear, \n    batch_size=batch_size,\n    validation_data=(X_valid_fear,y_valid_fear),\n    steps_per_epoch=len(X_train_fear) / batch_size,\n    epochs=epochs,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(x=history_fear.epoch, y=history_fear.history['accuracy'], label='train')\nsns.lineplot(x=history_fear.epoch, y=history_fear.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(x=history_fear.epoch, y=history_fear.history['loss'], label='train')\nsns.lineplot(x=history_fear.epoch, y=history_fear.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_fear_dcnn.png')\npyplot.show()\n\nprint(\"Final Accuracy of fear after 100 epochs will be: \", round(history_fear.history['accuracy'][99]*100,2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x_fear=fear_model.predict(X_valid_fear) \nyhat_valid_fear=np.argmax(predict_x_fear,axis=1)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid_fear, axis=1), yhat_valid_fear, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_fear_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid_fear, axis=1) != yhat_valid_fear)}\\n\\n')\nprint(classification_report(np.argmax(y_valid_fear, axis=1), yhat_valid_fear))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(fear_model, \"/kaggle/working/fear.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happy_model = build_net(optims[1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_happy = happy_model.fit(\n    X_train_happy, \n    y_train_happy, \n    batch_size=batch_size,\n    validation_data=(X_valid_happy,y_valid_happy),\n    steps_per_epoch=len(X_train_happy) / batch_size,\n    epochs=epochs,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(x=history_happy.epoch, y=history_happy.history['accuracy'], label='train')\nsns.lineplot(x=history_happy.epoch, y=history_happy.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(x=history_happy.epoch, y=history_happy.history['loss'], label='train')\nsns.lineplot(x=history_happy.epoch, y=history_happy.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_happy_dcnn.png')\npyplot.show()\n\nprint(\"Final Accuracy of happy after 100 epochs will be: \", round(history_happy.history['accuracy'][99]*100,2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x_happy=happy_model.predict(X_valid_happy) \nyhat_valid_happy=np.argmax(predict_x_happy,axis=1)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid_happy, axis=1), yhat_valid_happy, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_happy_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid_happy, axis=1) != yhat_valid_happy)}\\n\\n')\nprint(classification_report(np.argmax(y_valid_happy, axis=1), yhat_valid_happy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(happy_model, \"/kaggle/working/happy.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sad_model = build_net(optims[1]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_sad = sad_model.fit(\n    X_train_sad, \n    y_train_sad, \n    batch_size=batch_size,\n    validation_data=(X_valid_sad,y_valid_sad),\n    steps_per_epoch=len(X_train_sad) / batch_size,\n    epochs=epochs,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(x=history_sad.epoch, y=history_sad.history['accuracy'], label='train')\nsns.lineplot(x=history_sad.epoch, y=history_sad.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(x=history_sad.epoch, y=history_sad.history['loss'], label='train')\nsns.lineplot(x=history_sad.epoch, y=history_sad.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_sad_dcnn.png')\npyplot.show()\n\nprint(\"Final Accuracy of sad after 100 epochs will be: \", round(history_sad.history['accuracy'][99]*100,2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x_sad=sad_model.predict(X_valid_sad) \nyhat_valid_sad=np.argmax(predict_x_sad,axis=1)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid_sad, axis=1), yhat_valid_sad, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_sad_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid_sad, axis=1) != yhat_valid_sad)}\\n\\n')\nprint(classification_report(np.argmax(y_valid_sad, axis=1), yhat_valid_sad))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(sad_model, \"/kaggle/working/sad.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"suprise_model = build_net(optims[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_surprise = suprise_model.fit(\n    X_train_surprise, \n    y_train_surprise, \n    batch_size=batch_size,\n    validation_data=(X_valid_surprise,y_valid_surprise),\n    steps_per_epoch=len(X_train_surprise) / batch_size,\n    epochs=epochs,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(x=history_surprise.epoch, y=history_surprise.history['accuracy'], label='train')\nsns.lineplot(x=history_surprise.epoch, y=history_surprise.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(x=history_surprise.epoch, y=history_surprise.history['loss'], label='train')\nsns.lineplot(x=history_surprise.epoch, y=history_surprise.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_surprise_dcnn.png')\npyplot.show()\n\nprint(\"Final Accuracy of surprise after 100 epochs will be: \", round(history_surprise.history['accuracy'][99]*100,2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x_surprise=surprise_model.predict(X_valid_surprise) \nyhat_valid_surprise=np.argmax(predict_x_surprise,axis=1)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid_surprise, axis=1), yhat_valid_surprise, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_surprise_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid_surprise, axis=1) != yhat_valid_surprise)}\\n\\n')\nprint(classification_report(np.argmax(y_valid_suprise, axis=1), yhat_valid_surprise))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(suprise_model, \"/kaggle/working/surprise.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}